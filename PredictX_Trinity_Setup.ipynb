{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46a71a6",
   "metadata": {},
   "source": [
    "# ðŸ¤– PredictX: The Trinity Training (Tier 7)\n",
    "**Reinforcement Learning (PPO) + CNN Pattern Recognition + LSTM Trend Following**\n",
    "\n",
    "**âš ï¸ IMPORTANT: Validate that you have run the SETUP cell below before running any other cells!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518fbca",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Run this cell first to install dependencies and create the necessary folder structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602f15c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.14/site-packages (2.10.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.14/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.14/site-packages (2.4.2)\n",
      "Requirement already satisfied: ccxt in ./.venv/lib/python3.14/site-packages (4.5.37)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: stable-baselines3 in ./.venv/lib/python3.14/site-packages (2.7.1)\n",
      "Requirement already satisfied: shimmy in ./.venv/lib/python3.14/site-packages (2.0.0)\n",
      "Requirement already satisfied: gymnasium in ./.venv/lib/python3.14/site-packages (1.2.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.14/site-packages (3.10.8)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.14/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.14/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.14/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.14/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.14/site-packages (from torch) (2026.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in ./.venv/lib/python3.14/site-packages (from ccxt) (2026.1.4)\n",
      "Requirement already satisfied: requests>=2.18.4 in ./.venv/lib/python3.14/site-packages (from ccxt) (2.32.5)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in ./.venv/lib/python3.14/site-packages (from ccxt) (46.0.5)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in ./.venv/lib/python3.14/site-packages (from ccxt) (3.13.3)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in ./.venv/lib/python3.14/site-packages (from ccxt) (4.0.0)\n",
      "Requirement already satisfied: yarl>=1.7.2 in ./.venv/lib/python3.14/site-packages (from ccxt) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cloudpickle in ./.venv/lib/python3.14/site-packages (from stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv/lib/python3.14/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.14/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.14/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.14/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.14/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.14/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.14/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.14/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: pycares<6,>=5.0.0 in ./.venv/lib/python3.14/site-packages (from aiodns>=1.1.1->ccxt) (5.0.1)\n",
      "Requirement already satisfied: cffi>=2.0.0b1 in ./.venv/lib/python3.14/site-packages (from pycares<6,>=5.0.0->aiodns>=1.1.1->ccxt) (2.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.14/site-packages (from aiohttp>=3.10.11->ccxt) (0.4.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.14/site-packages (from yarl>=1.7.2->ccxt) (3.11)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.14/site-packages (from cffi>=2.0.0b1->pycares<6,>=5.0.0->aiodns>=1.1.1->ccxt) (3.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests>=2.18.4->ccxt) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests>=2.18.4->ccxt) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "âœ… Created backend/services\n",
      "âœ… Created backend/models\n",
      "âœ… Created backend/logs\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install torch pandas numpy ccxt scikit-learn stable-baselines3 shimmy gymnasium matplotlib\n",
    "\n",
    "# Create Directory Structure (Robust)\n",
    "import os\n",
    "dirs = ['backend/services', 'backend/models', 'backend/logs']\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f'âœ… Created {d}')\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('backend'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda07f8",
   "metadata": {},
   "source": [
    "## 2. Upload Codebase\n",
    "We write the Python files to the Colab environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540c43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def safe_write_file(path, content):\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    # Write file\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(content)\n",
    "    print(f'ðŸ“„ Written: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b07adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/services/data_service.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/services/data_service.py', r\"\"\"\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def get_historical_data(symbol: str, period: str = \"1mo\", interval: str = \"1h\", limit: int = 1000) -> dict:\n",
    "    \\\"\\\"\\\"\n",
    "    Fetch historical market data from Binance using CCXT.\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Ticker symbol (e.g., \"BTC-USD\", \"BTC/USDT\")\n",
    "        period (str): Ignored in CCXT (calculated from limit/since), kept for API compatibility.\n",
    "        interval (str): Data interval (default: \"1h\")\n",
    "        limit (int): Number of candles to fetch (default: 1000)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed data including candles and metadata\n",
    "    \\\"\\\"\\\"\n",
    "    print(f\"Fetching data for {symbol} (Interval: {interval}) using CCXT/Binance...\")\n",
    "\n",
    "    try:\n",
    "        # Initialize Binance Exchange\n",
    "        exchange = ccxt.binance()\n",
    "        \n",
    "        # Normalize symbol: CCXT expects \"BTC/USDT\", Frontend might send \"BTC-USD\" or \"BTCUSDT\"\n",
    "        # 1. Replace first hyphen with slash if exists (BTC-USD -> BTC/USD)\n",
    "        if \"-\" in symbol:\n",
    "            symbol = symbol.replace(\"-\", \"/\")\n",
    "        # 2. If no slash, assumes it might be raw (BTCUSDT), let CCXT try or manually fix if needed.\n",
    "        # But for \"BTC-USD\" from frontend, it usually means BTC/USDT in Binance terms.\n",
    "        if \"USD\" in symbol and \"USDT\" not in symbol:\n",
    "             symbol = symbol.replace(\"USD\", \"USDT\")\n",
    "        \n",
    "        # Ensure it has a slash for CCXT\n",
    "        if \"/\" not in symbol and len(symbol) > 6: \n",
    "             # Rough heuristic: insert slash before last 4 chars (USDT) or 3 chars (BTC)\n",
    "             # Better: just force standard map if known.\n",
    "             if symbol.endswith(\"USDT\"):\n",
    "                 symbol = symbol[:-4] + \"/USDT\"\n",
    "        \n",
    "        print(f\"Normalized symbol for Binance: {symbol}\")\n",
    "\n",
    "        # Map 'period' to limit/since if needed, but for now we'll fetch a fixed amount suitable for training\n",
    "        # If period is \"1mo\", 1h candles = 24 * 30 = 720 candles.\n",
    "        # Binance call limit is 1000.\n",
    "        fetch_limit = min(limit, 1000)\n",
    "        \n",
    "        # Fetch OHLCV\n",
    "        # timestamp, open, high, low, close, volume\n",
    "        ohlcv = exchange.fetch_ohlcv(symbol, timeframe=interval, limit=fetch_limit)\n",
    "        \n",
    "        if not ohlcv:\n",
    "             return {\"error\": f\"No data found for symbol {symbol}\"}\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(ohlcv, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        \n",
    "        # Process timestamps (ms to datetime string)\n",
    "        df['time'] = pd.to_datetime(df['time'], unit='ms').astype(str)\n",
    "        \n",
    "        return {\n",
    "            \"symbol\": symbol,\n",
    "            \"count\": len(df),\n",
    "            \"data\": df.to_dict(orient=\"records\")\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CCXT Error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a74c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/services/lstm_service.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/services/lstm_service.py', r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1)) # Ensure targets have correct shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
    "            \n",
    "    return history\n",
    "\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # data shape expected: (1, seq_len, input_size) or (seq_len, input_size)\n",
    "        if isinstance(data, list):\n",
    "            data = np.array(data)\n",
    "        \n",
    "        inputs = torch.tensor(data, dtype=torch.float32)\n",
    "        if inputs.dim() == 2:\n",
    "            inputs = inputs.unsqueeze(0) # Add batch dimension if missing\n",
    "            \n",
    "        output = model(inputs)\n",
    "        return output.item()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e94f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/services/cnn_service.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/services/cnn_service.py', r\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNPatternModel(nn.Module):\n",
    "    \\\"\\\"\\\"\n",
    "    CNN for Candlestick Pattern Recognition\n",
    "    Input: 20Ã—4 matrix (20 candles Ã— OHLC)\n",
    "    Output: Binary probability (Bullish/Bearish)\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, sequence_length=20, input_features=4):\n",
    "        super(CNNPatternModel, self).__init__()\n",
    "        \n",
    "        # Conv1D layers for pattern extraction\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_features, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Calculate flattened size after convolutions\n",
    "        # sequence_length=20 -> pool1(10) -> pool2(5)\n",
    "        self.flat_size = 128 * 5\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_size, 64)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Binary output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, features, sequence)\n",
    "        # Conv layers\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, self.flat_size)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))  # Output probability 0-1\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_cnn_model(model, train_loader, num_epochs=30, learning_rate=0.001):\n",
    "    \\\"\\\"\\\"\n",
    "    Train CNN pattern recognition model\n",
    "    \\\"\\\"\\\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_pattern(model, candle_window):\n",
    "    \\\"\\\"\\\"\n",
    "    Predict bullish/bearish pattern from candle window\n",
    "    Args:\n",
    "        candle_window: numpy array (20, 4) - OHLC data\n",
    "    Returns:\n",
    "        probability: float (0-1, where >0.5 = bullish)\n",
    "    \\\"\\\"\\\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor and reshape for CNN\n",
    "        # Shape: (1, features=4, sequence=20)\n",
    "        x = torch.FloatTensor(candle_window.T).unsqueeze(0)\n",
    "        prob = model(x).item()\n",
    "    return prob\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f21c6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/services/chart_generator.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/services/chart_generator.py', r\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def generate_chart_windows(df, window_size=20):\n",
    "    \\\"\\\"\\\"\n",
    "    Generate sliding windows of OHLC data for CNN training\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['open', 'high', 'low', 'close']\n",
    "        window_size: Number of candles per window\n",
    "    \n",
    "    Returns:\n",
    "        windows: numpy array (n_windows, window_size, 4)\n",
    "        labels: numpy array (n_windows, 1) - 1 if next candle is bullish, 0 if bearish\n",
    "    \\\"\\\"\\\"\n",
    "    windows = []\n",
    "    labels = []\n",
    "    \n",
    "    # Normalize OHLC data\n",
    "    scaler = MinMaxScaler()\n",
    "    ohlc_data = df[['open', 'high', 'low', 'close']].values\n",
    "    \n",
    "    for i in range(len(df) - window_size - 1):\n",
    "        # Extract window\n",
    "        window = ohlc_data[i:i+window_size]\n",
    "        \n",
    "        # Normalize window (0-1 range)\n",
    "        window_normalized = scaler.fit_transform(window)\n",
    "        \n",
    "        # Label: Is next candle bullish?\n",
    "        next_close = df.iloc[i + window_size]['close']\n",
    "        current_close = df.iloc[i + window_size - 1]['close']\n",
    "        label = 1.0 if next_close > current_close else 0.0\n",
    "        \n",
    "        windows.append(window_normalized)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(windows), np.array(labels).reshape(-1, 1)\n",
    "\n",
    "def detect_candlestick_patterns(window):\n",
    "    \\\"\\\"\\\"\n",
    "    Detect common candlestick patterns in a window\n",
    "    \n",
    "    Args:\n",
    "        window: numpy array (n, 4) - OHLC data\n",
    "    \n",
    "    Returns:\n",
    "        patterns: dict with pattern names and confidence scores\n",
    "    \\\"\\\"\\\"\n",
    "    patterns = {}\n",
    "    \n",
    "    # Get last candle\n",
    "    if len(window) < 1:\n",
    "        return patterns\n",
    "    \n",
    "    last = window[-1]\n",
    "    o, h, l, c = last[0], last[1], last[2], last[3]\n",
    "    \n",
    "    body = abs(c - o)\n",
    "    range_hl = h - l\n",
    "    \n",
    "    if range_hl == 0:\n",
    "        return patterns\n",
    "    \n",
    "    # Doji: Small body relative to range\n",
    "    if body / range_hl < 0.1:\n",
    "        patterns['doji'] = 0.8\n",
    "    \n",
    "    # Hammer: Long lower shadow, small body at top\n",
    "    lower_shadow = min(o, c) - l\n",
    "    upper_shadow = h - max(o, c)\n",
    "    if lower_shadow > 2 * body and upper_shadow < body:\n",
    "        patterns['hammer'] = 0.7\n",
    "    \n",
    "    # Engulfing (need 2 candles)\n",
    "    if len(window) >= 2:\n",
    "        prev = window[-2]\n",
    "        prev_o, prev_c = prev[0], prev[3]\n",
    "        \n",
    "        # Bullish engulfing\n",
    "        if prev_c < prev_o and c > o and c > prev_o and o < prev_c:\n",
    "            patterns['bullish_engulfing'] = 0.9\n",
    "        \n",
    "        # Bearish engulfing\n",
    "        if prev_c > prev_o and c < o and c < prev_o and o > prev_c:\n",
    "            patterns['bearish_engulfing'] = 0.9\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def prepare_cnn_input(candles, window_size=20):\n",
    "    \\\"\\\"\\\"\n",
    "    Prepare candle data for CNN input\n",
    "    \n",
    "    Args:\n",
    "        candles: list of dicts with OHLC data\n",
    "        window_size: Number of candles to use\n",
    "    \n",
    "    Returns:\n",
    "        numpy array (window_size, 4) ready for CNN\n",
    "    \\\"\\\"\\\"\n",
    "    if len(candles) < window_size:\n",
    "        return None\n",
    "    \n",
    "    # Extract last window_size candles\n",
    "    recent_candles = candles[-window_size:]\n",
    "    \n",
    "    # Convert to OHLC array\n",
    "    ohlc = np.array([\n",
    "        [c['open'], c['high'], c['low'], c['close']]\n",
    "        for c in recent_candles\n",
    "    ])\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    ohlc_normalized = scaler.fit_transform(ohlc)\n",
    "    \n",
    "    return ohlc_normalized\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6418f219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/ai_engine.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/ai_engine.py', r\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from services.lstm_service import LSTMModel, train_model, predict, TimeSeriesDataset\n",
    "from services.data_service import get_historical_data\n",
    "from services.db_service import db_service\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# RL Integration\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    RL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RL_AVAILABLE = False\n",
    "    print(\"âš ï¸ stable-baselines3 not installed. RL features disabled.\")\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    filename='training.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "def add_indicators(df):\n",
    "    \\\"\\\"\\\"\n",
    "    Feature Engineering: Adds Log Returns (Stationary), RSI, EMA Trend Difference, and EMA 200\n",
    "    \\\"\\\"\\\"\n",
    "    # 1. Log Returns (The Target)\n",
    "    # Log Change = ln(Pt / Pt-1)\n",
    "    # This makes the data stationary, which is crucial for LSTM\n",
    "    df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # EMA (Exponential Moving Average) - Trend Divergence\n",
    "    df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()\n",
    "    df['ema_diff'] = (df['close'] - df['ema_20']) / df['ema_20'] # Percentage distance to trend\n",
    "    \n",
    "    # EMA 200 - Major Trend Filter (Tier 5)\n",
    "    df['ema_200'] = df['close'].ewm(span=200, adjust=False).mean()\n",
    "    \n",
    "    # ATR (Average True Range) - Volatility\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift())\n",
    "    low_close = np.abs(df['low'] - df['close'].shift())\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = np.max(ranges, axis=1)\n",
    "    df['atr'] = true_range.rolling(14).mean()\n",
    "\n",
    "    # Fill NaN from rolling calculations\n",
    "    df.fillna(0, inplace=True) \n",
    "    return df\n",
    "\n",
    "class AIEngine:\n",
    "    def __init__(self):\n",
    "        self.models_loaded = False\n",
    "        print(\"Initializing AI Engine (Tier 5 - The Trend Surfer)...\")\n",
    "        logging.info(\"Initializing AI Engine (Tier 5 - The Trend Surfer)...\")\n",
    "        \n",
    "        # New Feature Set: Close, RSI, EMA_Diff\n",
    "        self.input_size = 3 \n",
    "        self.seq_length = 60\n",
    "        self.hidden_size = 128 \n",
    "        self.num_layers = 3    \n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler_path = 'models/scaler_v2.pkl'\n",
    "        self.model_path = 'models/predictx_v2.pth'\n",
    "        \n",
    "        # RL Agent (Tier 6)\n",
    "        self.rl_agent = None\n",
    "        self.rl_enabled = False\n",
    "        \n",
    "        # CNN Model (Tier 7)\n",
    "        self.cnn_model = None\n",
    "        self.cnn_enabled = False\n",
    "        \n",
    "        # Initialize LSTM Model\n",
    "        try:\n",
    "            self.lstm_model = LSTMModel(input_size=self.input_size, \n",
    "                                      hidden_size=self.hidden_size, \n",
    "                                      num_layers=self.num_layers)\n",
    "            self.load_model()\n",
    "            self.load_rl_agent()\n",
    "            self.load_cnn_model()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize LSTM: {e}\")\n",
    "            self.models_loaded = False\n",
    "\n",
    "    def load_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            try:\n",
    "                self.lstm_model.load_state_dict(torch.load(self.model_path))\n",
    "                self.lstm_model.eval()\n",
    "                \n",
    "                # Load Scaler if exists\n",
    "                if os.path.exists(self.scaler_path):\n",
    "                    self.scaler = joblib.load(self.scaler_path)\n",
    "                    \n",
    "                self.models_loaded = True\n",
    "                print(f\"LSTM Model (Tier 5) Loaded from {self.model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "        else:\n",
    "            print(\"LSTM Model Initialized (Random Weights) - Waiting for training\")\n",
    "\n",
    "    def prepare_data(self, data, seq_length):\n",
    "        xs, ys = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            x = data[i:(i + seq_length)]\n",
    "            # Predict only Close price (index 0) for next step\n",
    "            y = data[i + seq_length, 0] \n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.array(xs), np.array(ys)\n",
    "\n",
    "    def train(self, symbol=\"BTC-USD\", epochs=50, interval=\"1h\"):\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting Tier 5 training for {symbol} ({interval})...\")\n",
    "        logging.info(f\"Starting Tier 5 training session for {symbol} ({interval}) with {epochs} epochs\")\n",
    "        \n",
    "        # 1. Fetch Data\n",
    "        raw_data = get_historical_data(symbol, period=\"1y\", interval=interval)\n",
    "        if \"error\" in raw_data:\n",
    "            return {\"status\": \"error\", \"message\": raw_data[\"error\"]}\n",
    "            \n",
    "        df = pd.DataFrame(raw_data[\"data\"])\n",
    "        if df.empty:\n",
    "             return {\"status\": \"error\", \"message\": \"No data received\"}\n",
    "\n",
    "        # 2. Feature Engineering\n",
    "        df = add_indicators(df)\n",
    "        \n",
    "        # Select features: Log Return, RSI, EMA_Diff\n",
    "        # We replace 'close' with 'log_return' to prevent non-stationarity issues\n",
    "        features = df[['log_return', 'rsi', 'ema_diff']].values\n",
    "        \n",
    "        # 3. Scaling (Fit only on training data)\n",
    "        # For simplicity in this pipeline, we fit on the whole fetched dataset \n",
    "        # (assuming it's a batch update)\n",
    "        scaled_data = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Save scaler for inference\n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "        joblib.dump(self.scaler, self.scaler_path)\n",
    "        \n",
    "        # 4. Prepare Sequences\n",
    "        X, y = self.prepare_data(scaled_data, self.seq_length)\n",
    "        \n",
    "        # Split train/test\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        \n",
    "        train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # 5. Train\n",
    "        self.lstm_model.train()\n",
    "        history = train_model(self.lstm_model, train_loader, num_epochs=epochs)\n",
    "        \n",
    "        # 6. Save Model\n",
    "        torch.save(self.lstm_model.state_dict(), self.model_path)\n",
    "        self.models_loaded = True\n",
    "        \n",
    "        final_loss = history['loss'][-1] if history['loss'] else 0\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        print(f\"Training complete. Final Loss: {final_loss:.6f}\")\n",
    "        \n",
    "        # Log to DB\n",
    "        db_service.log_training_session({\n",
    "            \"symbol\": symbol,\n",
    "            \"epochs\": epochs,\n",
    "            \"final_loss\": float(final_loss),\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"duration_seconds\": round(duration, 2),\n",
    "            \"metadata\": {\"algorithm\": \"LSTM_Tier5_TrendSurfer\"}\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"message\": \"Tier 5 Model trained successfully\", \n",
    "            \"final_loss\": final_loss,\n",
    "            \"epochs\": epochs\n",
    "        }\n",
    "\n",
    "    def predict_next_move(self, candles: list):\n",
    "        \\\"\\\"\\\"\n",
    "        Returns probability of Uptrend next candle (0.0 - 1.0)\n",
    "        \\\"\\\"\\\"\n",
    "        # Need enough data for lag features (EMA 200 needs 200 candles)\n",
    "        if not self.models_loaded or len(candles) < 205:\n",
    "            return 0.5\n",
    "            \n",
    "        try:\n",
    "            df = pd.DataFrame(candles)\n",
    "            df = add_indicators(df)\n",
    "\n",
    "            # Take last seq_length features\n",
    "            current_features = df[['log_return', 'rsi', 'ema_diff']].tail(self.seq_length).values\n",
    "            \n",
    "            # Use saved scaler\n",
    "            scaled_input = self.scaler.transform(current_features)\n",
    "            \n",
    "            # Predict\n",
    "            self.lstm_model.eval()\n",
    "            with torch.no_grad():\n",
    "                # [1, seq, feat]\n",
    "                input_tensor = torch.FloatTensor(scaled_input).unsqueeze(0) \n",
    "                pred_scaled_return = self.lstm_model(input_tensor).item()\n",
    "            \n",
    "            # LOGIC: Return-based Probability\n",
    "            # pred_scaled_return is the predicted Z-Score of the next log return\n",
    "            # A positive value means price increase, negative means decrease\n",
    "            \n",
    "            # Normalize to 0-1 probability\n",
    "            # Sigmoid(x) centers at 0. Positive x -> >0.5\n",
    "            # We add a gain factor (e.g., 3) to make it more decisive\n",
    "            prob = 1 / (1 + np.exp(-pred_scaled_return * 3)) \n",
    "            \n",
    "            # --- TIER 5: TREND SURFER LOGIC ---\n",
    "            current_close = df['close'].iloc[-1]\n",
    "            ema_200 = df['ema_200'].iloc[-1]\n",
    "            current_rsi = df['rsi'].iloc[-1]\n",
    "            \n",
    "            # 1. Trend Filter (EMA 200)\n",
    "            # LONG ONLY if Price > EMA 200\n",
    "            if current_close > ema_200:\n",
    "                # Allow Bullish Prob. Suppress Bearish Prob.\n",
    "                if prob < 0.5:\n",
    "                    prob = 0.5 + (prob - 0.5) * 0.5 # Dampen bearish signal\n",
    "                # Boost Bullish Confidence slightly if strong trend\n",
    "                prob = min(0.99, prob * 1.05)\n",
    "            \n",
    "            # SHORT ONLY if Price < EMA 200\n",
    "            elif current_close < ema_200:\n",
    "                # Allow Bearish Prob. Suppress Bullish Prob.\n",
    "                if prob > 0.5:\n",
    "                    prob = 0.5 + (prob - 0.5) * 0.5 # Dampen bullish signal\n",
    "                # Boost Bearish Confidence slightly\n",
    "                prob = max(0.01, prob * 0.95)\n",
    "            \n",
    "            # 2. RSI Sanity Check (Relaxed)\n",
    "            # Prevent longing the absolute top (>85) or shorting absolute bottom (<15)\n",
    "            if current_rsi > 85: \n",
    "                prob = min(prob, 0.45) # Force Sell/Hold\n",
    "            if current_rsi < 15: \n",
    "                prob = max(prob, 0.55) # Force Buy/Hold\n",
    "            \n",
    "            return float(prob)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "    def load_rl_agent(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Load trained PPO agent for Tier 6 hybrid decision-making\n",
    "        \\\"\\\"\\\"\n",
    "        if not RL_AVAILABLE:\n",
    "            print(\"âš ï¸ RL agent not available (stable-baselines3 not installed)\")\n",
    "            return\n",
    "            \n",
    "        rl_model_path = \"models/ppo_agent.zip\"\n",
    "        if os.path.exists(rl_model_path):\n",
    "            try:\n",
    "                self.rl_agent = PPO.load(rl_model_path)\n",
    "                self.rl_enabled = True\n",
    "                print(f\"âœ… RL Agent (PPO) Loaded from {rl_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to load RL agent: {e}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ RL model not found at {rl_model_path}. Run train_rl_agent.py first.\")\n",
    "    \n",
    "    def get_rl_recommendation(self, state_vector):\n",
    "        \\\"\\\"\\\"\n",
    "        Get action recommendation from RL agent\n",
    "        Returns: (action_name, leverage, confidence)\n",
    "        \\\"\\\"\\\"\n",
    "        if not self.rl_enabled or self.rl_agent is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            action, _states = self.rl_agent.predict(state_vector, deterministic=True)\n",
    "            \n",
    "            # Map action to trading decision\n",
    "            action_map = {\n",
    "                0: (\"HOLD\", 1, 50),\n",
    "                1: (\"BUY\", 1, 70),\n",
    "                2: (\"BUY\", 3, 85),\n",
    "                3: (\"BUY\", 5, 95),\n",
    "                4: (\"SELL\", 1, 80)\n",
    "            }\n",
    "            \n",
    "            return action_map.get(action, (\"HOLD\", 1, 50))\n",
    "        except Exception as e:\n",
    "            print(f\"RL prediction error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_cnn_model(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Load trained CNN model for Tier 7 ensemble\n",
    "        \\\"\\\"\\\"\n",
    "        from services.cnn_service import CNNPatternModel\n",
    "        \n",
    "        cnn_model_path = \"models/cnn_pattern_v1.pth\"\n",
    "        if os.path.exists(cnn_model_path):\n",
    "            try:\n",
    "                self.cnn_model = CNNPatternModel(sequence_length=20, input_features=4)\n",
    "                self.cnn_model.load_state_dict(torch.load(cnn_model_path))\n",
    "                self.cnn_model.eval()\n",
    "                self.cnn_enabled = True\n",
    "                print(f\"âœ… CNN Pattern Model Loaded from {cnn_model_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to load CNN model: {e}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ CNN model not found at {cnn_model_path}. Run train_cnn.py first.\")\n",
    "    \n",
    "    def get_cnn_prediction(self, candles):\n",
    "        \\\"\\\"\\\"\n",
    "        Get pattern prediction from CNN model\n",
    "        Returns: probability (0-1, where >0.5 = bullish pattern)\n",
    "        \\\"\\\"\\\"\n",
    "        if not self.cnn_enabled or self.cnn_model is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from services.chart_generator import prepare_cnn_input\n",
    "            \n",
    "            # Prepare 20-candle window\n",
    "            window = prepare_cnn_input(candles, window_size=20)\n",
    "            if window is None:\n",
    "                return None\n",
    "            \n",
    "            # Get CNN prediction\n",
    "            from services.cnn_service import predict_pattern\n",
    "            prob = predict_pattern(self.cnn_model, window)\n",
    "            return prob\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"CNN prediction error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def decide_action(self, trend_prob: float, state_vector=None, candles=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Tier 7 (Trinity): LSTM + CNN + RL Ensemble\n",
    "        - LSTM: Trend Direction (Base)\n",
    "        - CNN: Pattern Recognition (Confirmation)\n",
    "        - RL: Strategic Decision (Entry/Exit + Leverage)\n",
    "        \\\"\\\"\\\"\n",
    "        # 1. Base Score (LSTM)\n",
    "        ensemble_score = trend_prob\n",
    "        \n",
    "        # 2. Add CNN Influence (if available)\n",
    "        cnn_prob = None\n",
    "        if self.cnn_enabled and candles is not None:\n",
    "            cnn_prob = self.get_cnn_prediction(candles)\n",
    "            if cnn_prob is not None:\n",
    "                # Weighted Fusion: LSTM 60%, CNN 40%\n",
    "                ensemble_score = (trend_prob * 0.6) + (cnn_prob * 0.4)\n",
    "        \n",
    "        # 3. Get RL Recommendation (if available)\n",
    "        rl_action = \"HOLD\"\n",
    "        rl_leverage = 1\n",
    "        rl_conf = 0\n",
    "        \n",
    "        if self.rl_enabled and state_vector is not None:\n",
    "            rl_decision = self.get_rl_recommendation(state_vector)\n",
    "            if rl_decision:\n",
    "                rl_action, rl_leverage, rl_conf = rl_decision\n",
    "        \n",
    "        # --- TRINITY FUSION LOGIC ---\n",
    "        \n",
    "        confidence = abs(ensemble_score - 0.5) * 2 * 100\n",
    "        \n",
    "        # CASE A: BUY SIGNAL\n",
    "        if ensemble_score > 0.55: # Bullish Trend\n",
    "            # If RL agrees (BUY), confidence boosted\n",
    "            if \"BUY\" in rl_action:\n",
    "                final_leverage = rl_leverage\n",
    "                return f\"BUY_{final_leverage}x\", min(99, confidence + 15)\n",
    "            # If RL says HOLD, weak buy (1x)\n",
    "            elif rl_action == \"HOLD\":\n",
    "                return \"BUY_1x\", confidence\n",
    "            # If RL says SELL, conflict -> HOLD\n",
    "            elif rl_action == \"SELL\":\n",
    "                return \"HOLD\", confidence\n",
    "                \n",
    "        # CASE B: SELL SIGNAL\n",
    "        elif ensemble_score < 0.45: # Bearish Trend\n",
    "            # If RL agrees (SELL), strong exit\n",
    "            if rl_action == \"SELL\":\n",
    "                return \"SELL\", min(99, confidence + 15)\n",
    "            # If RL says HOLD/BUY, but Trend is Bearish -> Weak Exit\n",
    "            else:\n",
    "                return \"SELL\", confidence\n",
    "                \n",
    "        # CASE C: NEUTRAL CHART (Side-ways)\n",
    "        else:\n",
    "            # RL is the Tie-Breaker\n",
    "            if \"BUY\" in rl_action and confidence > 40:\n",
    "                return f\"BUY_{rl_leverage}x\", 50 # Speculative Buy\n",
    "            elif rl_action == \"SELL\":\n",
    "                return \"SELL\", 50 # Speculative Exit\n",
    "                \n",
    "        return \"HOLD\", round(confidence, 1)\n",
    "\n",
    "ai_engine = AIEngine()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2fed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/rl_trading_env.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/rl_trading_env.py', r\"\"\"\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ai_engine import add_indicators\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    \\\"\\\"\\\"\n",
    "    Custom Trading Environment for RL Agent (PPO)\n",
    "    Optimized for Rp 250k capital with leverage support\n",
    "    \\\"\\\"\\\"\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df, initial_balance=250000, fee_rate=0.001):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        \n",
    "        self.df = df\n",
    "        self.initial_balance = initial_balance\n",
    "        self.fee_rate = fee_rate\n",
    "        \n",
    "        # State: [Close_norm, RSI, EMA_Diff, LSTM_Prob, Position, Balance_norm, Leverage]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -1, 0, -1, 0, 0]),\n",
    "            high=np.array([1, 100, 1, 1, 1, 10, 5]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action: 0=Hold, 1=Buy_1x, 2=Buy_3x, 3=Buy_5x, 4=Sell\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.current_step = 60  # Start after enough data for indicators\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0.0\n",
    "        self.entry_price = 0.0\n",
    "        self.leverage = 1\n",
    "        self.total_profit = 0\n",
    "        self.trades = []\n",
    "        self.equity_curve = [self.initial_balance]\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \\\"\\\"\\\"\n",
    "        Returns normalized state vector\n",
    "        \\\"\\\"\\\"\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        \n",
    "        # Normalize close price (0-1 range based on recent window)\n",
    "        recent_prices = self.df['close'].iloc[max(0, self.current_step-100):self.current_step+1]\n",
    "        close_norm = (row['close'] - recent_prices.min()) / (recent_prices.max() - recent_prices.min() + 1e-8)\n",
    "        \n",
    "        # RSI (already 0-100)\n",
    "        rsi = row['rsi']\n",
    "        \n",
    "        # EMA Diff (already percentage)\n",
    "        ema_diff = row['ema_diff']\n",
    "        \n",
    "        # LSTM Probability (placeholder - will be filled by ai_engine)\n",
    "        lstm_prob = 0.5  # Neutral default\n",
    "        \n",
    "        # Position (-1=short, 0=flat, 1=long)\n",
    "        position_state = 1 if self.position > 0 else 0\n",
    "        \n",
    "        # Balance normalized (relative to initial)\n",
    "        balance_norm = self.balance / self.initial_balance\n",
    "        \n",
    "        # Current leverage\n",
    "        leverage_state = self.leverage\n",
    "        \n",
    "        return np.array([\n",
    "            close_norm,\n",
    "            rsi / 100.0,  # Normalize to 0-1\n",
    "            ema_diff,\n",
    "            lstm_prob,\n",
    "            position_state,\n",
    "            balance_norm,\n",
    "            leverage_state / 5.0  # Normalize to 0-1\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \\\"\\\"\\\"\n",
    "        Execute action and return (observation, reward, done, truncated, info)\n",
    "        \\\"\\\"\\\"\n",
    "        # Convert numpy array to int if needed\n",
    "        if hasattr(action, 'item'):\n",
    "            action = action.item()\n",
    "        \n",
    "        current_price = self.df.iloc[self.current_step]['close']\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # Action Mapping\n",
    "        # 0 = Hold\n",
    "        # 1 = Buy 1x\n",
    "        # 2 = Buy 3x\n",
    "        # 3 = Buy 5x\n",
    "        # 4 = Sell\n",
    "        \n",
    "        # Execute Action\n",
    "        if action in [1, 2, 3] and self.position == 0 and self.balance > 10000:  # Min Rp 10k per trade\n",
    "            # BUY with leverage\n",
    "            leverage_map = {1: 1, 2: 3, 3: 5}\n",
    "            self.leverage = leverage_map[action]\n",
    "            \n",
    "            # Calculate position size (use 20% of balance for risk management)\n",
    "            trade_amount = self.balance * 0.2 * self.leverage\n",
    "            fee = trade_amount * self.fee_rate\n",
    "            net_amount = trade_amount - fee\n",
    "            \n",
    "            self.position = net_amount / current_price\n",
    "            self.entry_price = current_price\n",
    "            self.balance -= (trade_amount / self.leverage)  # Deduct margin\n",
    "            \n",
    "            self.trades.append({\n",
    "                'type': 'BUY',\n",
    "                'price': current_price,\n",
    "                'leverage': self.leverage,\n",
    "                'step': self.current_step\n",
    "            })\n",
    "            \n",
    "        elif action == 4 and self.position > 0:\n",
    "            # SELL\n",
    "            sell_value = self.position * current_price\n",
    "            fee = sell_value * self.fee_rate\n",
    "            net_value = sell_value - fee\n",
    "            \n",
    "            # Calculate P&L\n",
    "            profit = net_value - (self.entry_price * self.position)\n",
    "            self.total_profit += profit\n",
    "            \n",
    "            # Return margin + profit\n",
    "            self.balance += (self.entry_price * self.position / self.leverage) + profit\n",
    "            \n",
    "            self.trades.append({\n",
    "                'type': 'SELL',\n",
    "                'price': current_price,\n",
    "                'profit': profit,\n",
    "                'step': self.current_step\n",
    "            })\n",
    "            \n",
    "            # Calculate reward based on profit percentage\n",
    "            profit_pct = profit / (self.entry_price * self.position)\n",
    "            reward = profit_pct * 100  # Scale reward\n",
    "            \n",
    "            # Reset position\n",
    "            self.position = 0\n",
    "            self.entry_price = 0\n",
    "            self.leverage = 1\n",
    "        \n",
    "        # Check Liquidation (if price moves against us by 1/leverage)\n",
    "        if self.position > 0:\n",
    "            liquidation_price = self.entry_price * (1 - 0.9 / self.leverage)\n",
    "            if current_price <= liquidation_price:\n",
    "                # LIQUIDATED\n",
    "                self.balance = 0\n",
    "                self.position = 0\n",
    "                reward = -100  # Heavy penalty\n",
    "                done = True\n",
    "        \n",
    "        # Update equity curve\n",
    "        current_equity = self.balance + (self.position * current_price if self.position > 0 else 0)\n",
    "        self.equity_curve.append(current_equity)\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Episode end conditions\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            done = True\n",
    "            # Final reward based on total return\n",
    "            final_return = (current_equity - self.initial_balance) / self.initial_balance\n",
    "            reward += final_return * 100\n",
    "        \n",
    "        # Bankruptcy check\n",
    "        if self.balance < 5000:  # Below minimum tradeable amount\n",
    "            done = True\n",
    "            reward = -50\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        info = {\n",
    "            'balance': self.balance,\n",
    "            'position': self.position,\n",
    "            'total_profit': self.total_profit,\n",
    "            'trades': len(self.trades)\n",
    "        }\n",
    "        \n",
    "        return observation, reward, done, False, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        current_equity = self.balance + (self.position * self.df.iloc[self.current_step]['close'] if self.position > 0 else 0)\n",
    "        print(f\"Step: {self.current_step} | Balance: Rp {self.balance:,.0f} | Equity: Rp {current_equity:,.0f} | Trades: {len(self.trades)}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e3551a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/train_cnn.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/train_cnn.py', r\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from services.cnn_service import CNNPatternModel, train_cnn_model\n",
    "from services.chart_generator import generate_chart_windows\n",
    "from services.data_service import get_historical_data\n",
    "from ai_engine import add_indicators\n",
    "\n",
    "def train_cnn_pattern_model(epochs=40):\n",
    "    \\\"\\\"\\\"\n",
    "    Train CNN model for candlestick pattern recognition using multiple assets\n",
    "    \\\"\\\"\\\"\n",
    "    symbols = [\"BTC-USD\", \"ETH-USD\", \"BNB-USD\", \"SOL-USD\", \"ADA-USD\"]\n",
    "    print(f\"ðŸ§  Training CNN Pattern Model for {len(symbols)} symbols\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_windows = []\n",
    "    all_labels = []\n",
    "\n",
    "    for symbol in symbols:\n",
    "        # 1. Fetch Historical Data\n",
    "        print(f\"\\n[1/5] Fetching data for {symbol}...\")\n",
    "        raw_data = get_historical_data(symbol, period=\"2y\", interval=\"1h\", limit=1000)\n",
    "        \n",
    "        if \"error\" in raw_data:\n",
    "            print(f\"   âŒ Error: {raw_data['error']}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.DataFrame(raw_data[\"data\"])\n",
    "        df = add_indicators(df)\n",
    "        print(f\"   âœ… Loaded {len(df)} candles\")\n",
    "        \n",
    "        # 2. Generate Training Windows\n",
    "        windows, labels = generate_chart_windows(df, window_size=20)\n",
    "        print(f\"   âœ… Generated {len(windows)} training samples\")\n",
    "        \n",
    "        all_windows.append(windows)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    if not all_windows:\n",
    "        print(\"âŒ No data loaded. Aborting.\")\n",
    "        return\n",
    "\n",
    "    windows = np.concatenate(all_windows, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    print(f\"\\nâœ¨ TOTAL SAMPLES: {len(windows)}\")\n",
    "    \n",
    "    # 3. Split Train/Test\n",
    "    # Shuffle first to mix symbols\n",
    "    indices = np.arange(len(windows))\n",
    "    np.random.shuffle(indices)\n",
    "    windows = windows[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    split_idx = int(len(windows) * 0.8)\n",
    "    train_windows = windows[:split_idx]\n",
    "    train_labels = labels[:split_idx]\n",
    "    test_windows = windows[split_idx:]\n",
    "    test_labels = labels[split_idx:]\n",
    "    \n",
    "    print(f\"   Train: {len(train_windows)} | Test: {len(test_windows)}\")\n",
    "    \n",
    "    # 4. Create DataLoaders\n",
    "    print(\"\\n[3/5] Preparing data loaders...\")\n",
    "    \n",
    "    # Convert to tensors (batch, features, sequence)\n",
    "    train_x = torch.FloatTensor(train_windows).permute(0, 2, 1)  # (N, 4, 20)\n",
    "    train_y = torch.FloatTensor(train_labels)\n",
    "    test_x = torch.FloatTensor(test_windows).permute(0, 2, 1)\n",
    "    test_y = torch.FloatTensor(test_labels)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 5. Initialize Model\n",
    "    print(\"\\n[4/5] Training CNN model...\")\n",
    "    model = CNNPatternModel(sequence_length=20, input_features=4)\n",
    "    \n",
    "    # Train\n",
    "    history = train_cnn_model(model, train_loader, num_epochs=epochs, learning_rate=0.001)\n",
    "    \n",
    "    # 6. Evaluate on Test Set\n",
    "    print(\"\\n[5/5] Evaluating on test set...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"âœ… Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # 7. Save Model\n",
    "    model_path = \"models/cnn_pattern_v2.pth\"\n",
    "    # Also overwrite the active model path if it exists or use v1 as default\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(model.state_dict(), \"models/cnn_pattern_v1.pth\") # Active model\n",
    "    print(f\"\\nâœ… Model saved to {model_path} and models/cnn_pattern_v1.pth\")\n",
    "    \n",
    "    # 8. Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Symbols     : {len(symbols)}\")\n",
    "    print(f\"Total Samples     : {len(windows)}\")\n",
    "    print(f\"Final Train Acc   : {history['accuracy'][-1]:.2f}%\")\n",
    "    print(f\"Test Accuracy     : {test_accuracy:.2f}%\")\n",
    "    print(f\"Epochs            : {epochs}\")\n",
    "    \n",
    "    if test_accuracy > 60:\n",
    "        print(\"\\nâœ… Status: READY FOR ENSEMBLE\")\n",
    "    elif test_accuracy > 52:\n",
    "        print(\"\\nâš ï¸ Status: ACCEPTABLE (Noisy data)\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Status: NEEDS IMPROVEMENT\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cnn_pattern_model(epochs=40)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2a8482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/train_rl_agent.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/train_rl_agent.py', r\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from rl_trading_env import TradingEnv\n",
    "from services.data_service import get_historical_data\n",
    "from ai_engine import add_indicators\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \\\"\\\"\\\"\n",
    "    Custom callback for logging training metrics\n",
    "    \\\"\\\"\\\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log episode rewards\n",
    "        if len(self.locals.get('infos', [])) > 0:\n",
    "            for info in self.locals['infos']:\n",
    "                if 'episode' in info:\n",
    "                    self.episode_rewards.append(info['episode']['r'])\n",
    "                    self.episode_lengths.append(info['episode']['l'])\n",
    "                    \n",
    "                    if len(self.episode_rewards) % 10 == 0:\n",
    "                        avg_reward = np.mean(self.episode_rewards[-10:])\n",
    "                        print(f\"Episode {len(self.episode_rewards)} | Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "def train_rl_agent(symbol=\"BTC-USD\", total_timesteps=100000):\n",
    "    \\\"\\\"\\\"\n",
    "    Train PPO agent on historical trading data\n",
    "    \\\"\\\"\\\"\n",
    "    print(f\"ðŸ¤– Starting RL Agent Training for {symbol}\")\n",
    "    print(f\"Total Timesteps: {total_timesteps:,}\")\n",
    "    \n",
    "    # 1. Fetch Historical Data (2 years for better training)\n",
    "    print(\"\\n[1/4] Fetching historical data...\")\n",
    "    raw_data = get_historical_data(symbol, period=\"2y\", interval=\"1h\")\n",
    "    \n",
    "    if \"error\" in raw_data:\n",
    "        print(f\"âŒ Error: {raw_data['error']}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(raw_data[\"data\"])\n",
    "    if df.empty:\n",
    "        print(\"âŒ No data received\")\n",
    "        return\n",
    "    \n",
    "    # 2. Add Indicators\n",
    "    print(\"[2/4] Adding technical indicators...\")\n",
    "    df = add_indicators(df)\n",
    "    print(f\"âœ… Prepared {len(df)} candles with indicators\")\n",
    "    \n",
    "    # 3. Create Environment\n",
    "    print(\"[3/4] Creating trading environment...\")\n",
    "    env = TradingEnv(df, initial_balance=250000)\n",
    "    env = DummyVecEnv([lambda: env])  # Vectorize for SB3\n",
    "    \n",
    "    # 4. Initialize PPO Agent\n",
    "    print(\"[4/4] Training PPO agent...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=0.0003,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    callback = TensorboardCallback()\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    \n",
    "    # 5. Save Model\n",
    "    model_path = \"models/ppo_agent\"\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nâœ… Model saved to {model_path}.zip\")\n",
    "    \n",
    "    # 6. Quick Evaluation\n",
    "    print(\"\\n--- Quick Evaluation ---\")\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 1000:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward[0]\n",
    "        steps += 1\n",
    "    \n",
    "    print(f\"Evaluation Steps: {steps}\")\n",
    "    print(f\"Total Reward: {total_reward:.2f}\")\n",
    "    print(f\"Final Balance: Rp {info[0]['balance']:,.0f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train with 100k timesteps (adjust based on compute power)\n",
    "    # For faster testing, use 10000. For production, use 500000+\n",
    "    train_rl_agent(symbol=\"BTC-USD\", total_timesteps=50000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0b042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Written: backend/services/backtest_service.py\n"
     ]
    }
   ],
   "source": [
    "safe_write_file('backend/services/backtest_service.py', r\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from services.data_service import get_historical_data\n",
    "\n",
    "def calculate_max_drawdown(balances):\n",
    "    \\\"\\\"\\\"\n",
    "    Menghitung persentase penurunan terdalam dari titik puncak (Peak)\n",
    "    \\\"\\\"\\\"\n",
    "    if not balances:\n",
    "        return 0\n",
    "    \n",
    "    # Konversi ke numpy array untuk perhitungan cepat\n",
    "    equity_curve = np.array(balances)\n",
    "    \n",
    "    # Hitung running maximum (titik tertinggi sejauh ini)\n",
    "    peak = np.maximum.accumulate(equity_curve)\n",
    "    \n",
    "    # Hitung drawdown (selisih dari peak)\n",
    "    # Avoid division by zero\n",
    "    drawdown = np.zeros_like(peak)\n",
    "    mask = peak > 0\n",
    "    drawdown[mask] = (equity_curve[mask] - peak[mask]) / peak[mask]\n",
    "    \n",
    "    # Ambil nilai minimum (penurunan paling dalam)\n",
    "    max_drawdown = np.min(drawdown) if len(drawdown) > 0 else 0\n",
    "    \n",
    "    return max_drawdown * 100 # Dalam persen\n",
    "\n",
    "def calculate_detailed_metrics(item_equity, trades, df, initial_balance):\n",
    "    \\\"\\\"\\\"\n",
    "    Menghitung metrik backtest komprehensif.\n",
    "    \\\"\\\"\\\"\n",
    "    if df.empty or not trades:\n",
    "        return {}\n",
    "\n",
    "    # Basic Data\n",
    "    start_price = df.iloc[0]['close']\n",
    "    end_price = df.iloc[-1]['close']\n",
    "    start_time = pd.to_datetime(df.iloc[0]['time'])\n",
    "    end_time = pd.to_datetime(df.iloc[-1]['time'])\n",
    "    duration_days = (end_time - start_time).days\n",
    "    duration_years = duration_days / 365.25\n",
    "    duration_months = duration_days / 30.44\n",
    "\n",
    "    final_equity = item_equity[-1] if item_equity else initial_balance\n",
    "    total_return_abs = final_equity - initial_balance\n",
    "    agent_return_pct = (total_return_abs / initial_balance) * 100\n",
    "    \n",
    "    # 1. Buy & Hold Return\n",
    "    buy_hold_return = ((end_price - start_price) / start_price) * 100\n",
    "    \n",
    "    # 2. Sell & Hold Return (Inverse)\n",
    "    sell_hold_return = ((start_price - end_price) / start_price) * 100\n",
    "    \n",
    "    # 3. Process Trades for Win/Loss/Duration\n",
    "    wins = []\n",
    "    losses = []\n",
    "    durations = []\n",
    "    \n",
    "    # Reconstruct paired trades\n",
    "    current_buy = None\n",
    "    \n",
    "    for t in trades:\n",
    "        if 'BUY' in t['type']:\n",
    "            current_buy = t\n",
    "        elif 'SELL' in t['type'] and current_buy:\n",
    "            pnl = t['balance'] - current_buy['balance']\n",
    "            \n",
    "            # Duration\n",
    "            t_time = pd.to_datetime(t['time'])\n",
    "            o_time = pd.to_datetime(current_buy['time'])\n",
    "            duration_mins = (t_time - o_time).total_seconds() / 60\n",
    "            durations.append(duration_mins)\n",
    "            \n",
    "            if pnl > 0:\n",
    "                wins.append(pnl)\n",
    "            else:\n",
    "                losses.append(abs(pnl))\n",
    "            \n",
    "            current_buy = None\n",
    "\n",
    "    total_trades = len(wins) + len(losses)\n",
    "    win_rate = (len(wins) / total_trades * 100) if total_trades > 0 else 0\n",
    "    \n",
    "    avg_win = np.mean(wins) if wins else 0\n",
    "    avg_loss = np.mean(losses) if losses else 0\n",
    "    \n",
    "    # Risk:Reward Ratio\n",
    "    risk_reward = (avg_win / avg_loss) if avg_loss > 0 else 0\n",
    "    \n",
    "    # Profit Factor\n",
    "    gross_profit = sum(wins)\n",
    "    gross_loss = sum(losses)\n",
    "    profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else 0\n",
    "    \n",
    "    # Expectancy (R)\n",
    "    expectancy = (len(wins)/total_trades * avg_win) - (len(losses)/total_trades * avg_loss) if total_trades > 0 else 0\n",
    "\n",
    "    # Avg Profit / Month\n",
    "    avg_profit_month = total_return_abs / duration_months if duration_months > 0 else 0\n",
    "\n",
    "    # Time in Market\n",
    "    total_time_in_market_min = sum(durations)\n",
    "    total_period_min = duration_days * 24 * 60\n",
    "    time_in_market_pct = (total_time_in_market_min / total_period_min * 100) if total_period_min > 0 else 0\n",
    "    \n",
    "    # 4. Returns & Ratios (Sharpe, Calmar)\n",
    "    equity_series = pd.Series(item_equity)\n",
    "    returns = equity_series.pct_change().dropna()\n",
    "    \n",
    "    # Sharpe Ratio (Annualized) - Assuming hourly data approx\n",
    "    n_periods = 252 * 24 \n",
    "    if returns.std() != 0:\n",
    "        sharpe = (returns.mean() / returns.std()) * np.sqrt(n_periods)\n",
    "    else:\n",
    "        sharpe = 0\n",
    "        \n",
    "    # Max Drawdown\n",
    "    mdd_pct = calculate_max_drawdown(item_equity)\n",
    "    \n",
    "    # Calmar Ratio\n",
    "    annualized_return_pct = ((final_equity / initial_balance) ** (365/duration_days) - 1) * 100 if duration_days > 0 else 0\n",
    "    calmar = abs(annualized_return_pct / mdd_pct) if mdd_pct != 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"agent_return\": agent_return_pct,\n",
    "        \"avg_profit_month\": avg_profit_month,\n",
    "        \"buy_hold_return\": buy_hold_return,\n",
    "        \"sell_hold_return\": sell_hold_return,\n",
    "        \"sharpe_ratio\": sharpe,\n",
    "        \"calmar_ratio\": calmar,\n",
    "        \"profit_factor\": profit_factor,\n",
    "        \"expectancy\": expectancy,\n",
    "        \"risk_reward\": risk_reward,\n",
    "        \"max_drawdown\": mdd_pct,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_win\": avg_win,\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"time_in_market\": time_in_market_pct,\n",
    "        \"total_trades\": total_trades,\n",
    "        \"buy_count\": len([t for t in trades if 'BUY' in t['type']]),\n",
    "        \"sell_count\": len([t for t in trades if 'SELL' in t['type']])\n",
    "    }\n",
    "\n",
    "def print_metrics_report(metrics):\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"BACKTEST PERFORMANCE REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"{'Agent Return':<25}: {metrics.get('agent_return', 0):.2f}%\")\n",
    "    print(f\"{'Avg Profit/Month':<25}: ${metrics.get('avg_profit_month', 0):.2f}\")\n",
    "    print(f\"{'Buy & Hold Return':<25}: {metrics.get('buy_hold_return', 0):.2f}%\")\n",
    "    print(f\"{'Sell & Hold Return':<25}: {metrics.get('sell_hold_return', 0):.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Sharpe Ratio':<25}: {metrics.get('sharpe_ratio', 0):.2f}\")\n",
    "    print(f\"{'Calmar Ratio':<25}: {metrics.get('calmar_ratio', 0):.2f}\")\n",
    "    print(f\"{'Profit Factor':<25}: {metrics.get('profit_factor', 0):.2f}\")\n",
    "    print(f\"{'Expectancy (R)':<25}: ${metrics.get('expectancy', 0):.2f}\")\n",
    "    print(f\"{'Risk:Reward Ratio':<25}: {metrics.get('risk_reward', 0):.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Max Drawdown':<25}: {metrics.get('max_drawdown', 0):.2f}%\")\n",
    "    print(f\"{'Win Rate':<25}: {metrics.get('win_rate', 0):.2f}%\")\n",
    "    print(f\"{'Avg Win':<25}: ${metrics.get('avg_win', 0):.2f}\")\n",
    "    print(f\"{'Avg Loss':<25}: ${metrics.get('avg_loss', 0):.2f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'Time in Market':<25}: {metrics.get('time_in_market', 0):.2f}%\")\n",
    "    print(f\"{'Total Trades':<25}: {metrics.get('total_trades', 0)}\")\n",
    "    print(f\"{'Orders (BUY:SELL)':<25}: {metrics.get('buy_count', 0)} : {metrics.get('sell_count', 0)}\")\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "def plot_backtest_results(df, history):\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        balances = [t['balance'] for t in history if 'balance' in t]\n",
    "        if not balances:\n",
    "            print(\"No balance history to plot\")\n",
    "            return\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(len(balances)), balances, label='Equity ($)', color='green')\n",
    "        plt.title('Backtest Equity Curve')\n",
    "        plt.xlabel('Trade #')\n",
    "        plt.ylabel('Balance ($)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.savefig('backtest_equity.png')\n",
    "        print(\"ðŸ“ˆ Equity curve saved to 'backtest_equity.png'\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ Matplotlib not installed. Skipping Graph.\")\n",
    "\n",
    "def run_backtest_v2(engine, symbol=\"BTC-USD\", period=\"3mo\", interval=\"1h\"):\n",
    "    print(f\"Strategy: The Trinity Hunter (Tier 7 - RL+CNN+LSTM)\")\n",
    "    \n",
    "    # 1. Fetch Historical Data\n",
    "    raw_data = get_historical_data(symbol, period=period, interval=interval)\n",
    "    \n",
    "    if \"error\" in raw_data:\n",
    "        print(f\"âŒ Error fetching data: {raw_data['error']}\")\n",
    "        return 0, [], pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(raw_data[\"data\"])\n",
    "    if df.empty:\n",
    "        print(\"âŒ No data received.\")\n",
    "        return 0, [], pd.DataFrame()\n",
    "\n",
    "    # --- FEATURE ENGINEERING ---\n",
    "    from ai_engine import add_indicators\n",
    "    df = add_indicators(df)\n",
    "    print(f\"âœ… Loaded {len(df)} candles with Indicators.\")\n",
    "\n",
    "    initial_balance = 1000.0\n",
    "    balance = initial_balance\n",
    "    position = 0.0\n",
    "    trades = []\n",
    "    \n",
    "    fee_rate = 0.001\n",
    "    entry_price = 0.0\n",
    "    \n",
    "    # Buffer needed for indicators (200 for EMA200) + sequence length\n",
    "    start_idx = max(engine.seq_length + 20, 205)\n",
    "    \n",
    "    equity_history = []\n",
    "    \n",
    "    # Pre-fill equity history for accurate metrics\n",
    "    for _ in range(start_idx):\n",
    "        equity_history.append(initial_balance)\n",
    "    \n",
    "    print(\"â³ Running simulation (Trinity Ensemble)...\")\n",
    "    \n",
    "    for i in range(start_idx, len(df)):\n",
    "        current_window_df = df.iloc[:i]\n",
    "        current_candles = current_window_df.to_dict('records')\n",
    "        \n",
    "        current_price = float(df.iloc[i]['close'])\n",
    "        current_low = float(df.iloc[i]['low'])\n",
    "        current_high = float(df.iloc[i]['high'])\n",
    "        timestamp = df.iloc[i]['time']\n",
    "        \n",
    "        executed_trade = False\n",
    "        \n",
    "        # 1. Check Open Position (Exit Logic)\n",
    "        if position > 0:\n",
    "            pnl_pct = (current_price - entry_price) / entry_price\n",
    "            \n",
    "            # Dynamic TP/SL or Signal Exit?\n",
    "            # Basic Safety TP/SL\n",
    "            tp_hit = current_high >= entry_price * 1.06\n",
    "            sl_hit = current_low <= entry_price * 0.95 # Looser SL for Swing\n",
    "            \n",
    "            # Ask AI for Exit Signal (Trinity)\n",
    "            # We need to construct State Vector for RL\n",
    "            prob = engine.predict_next_move(current_candles)\n",
    "            \n",
    "            # Construct State Vector [close_norm, rsi, ema, lstm_prob, pos, bal, lev]\n",
    "            recent_prices = df['close'].iloc[max(0, i-100):i+1]\n",
    "            if recent_prices.max() == recent_prices.min():\n",
    "                 close_norm = 0.5\n",
    "            else:\n",
    "                 close_norm = (current_price - recent_prices.min()) / (recent_prices.max() - recent_prices.min())\n",
    "            \n",
    "            rsi_norm = df.iloc[i]['rsi'] / 100.0\n",
    "            ema_diff = df.iloc[i]['ema_diff']\n",
    "            pos_state = 1\n",
    "            bal_norm = balance / initial_balance # Approximation\n",
    "            \n",
    "            # RL State\n",
    "            state_vector = np.array([\n",
    "               close_norm, rsi_norm, ema_diff, prob, pos_state, bal_norm, 0.2\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            # Fix State Vector shape issues if any\n",
    "            if state_vector.shape != (7,):\n",
    "                 state_vector = np.zeros(7, dtype=np.float32)\n",
    "\n",
    "            action_code, confidence = engine.decide_action(prob, state_vector, current_candles)\n",
    "            \n",
    "            exit_reason = \"\"\n",
    "            exit_price = current_price\n",
    "            \n",
    "            if tp_hit:\n",
    "                exit_reason = \"TP (+6%)\"\n",
    "                exit_price = entry_price * 1.06\n",
    "            elif sl_hit:\n",
    "                exit_reason = \"SL (-5%)\"\n",
    "                exit_price = entry_price * 0.95\n",
    "            elif \"SELL\" in action_code:\n",
    "                # Only exit on Sell signal if confidence is decent\n",
    "                if confidence > 45:\n",
    "                     exit_reason = f\"AI Sell ({confidence}%)\"\n",
    "                     exit_price = current_price\n",
    "            \n",
    "            if exit_reason:\n",
    "                sell_val = position * exit_price\n",
    "                fee = sell_val * fee_rate\n",
    "                balance = sell_val - fee\n",
    "                trades.append({\n",
    "                    \"time\": timestamp, \"type\": \"SELL\", \"price\": exit_price,\n",
    "                    \"balance\": balance, \"reason\": exit_reason\n",
    "                })\n",
    "                position = 0\n",
    "                entry_price = 0\n",
    "                executed_trade = True\n",
    "\n",
    "        # 2. Check Entry\n",
    "        if not executed_trade and position == 0:\n",
    "            prob = engine.predict_next_move(current_candles)\n",
    "            \n",
    "            # Construct State Vector\n",
    "            recent_prices = df['close'].iloc[max(0, i-100):i+1]\n",
    "            if recent_prices.max() == recent_prices.min():\n",
    "                 close_norm = 0.5\n",
    "            else:\n",
    "                 close_norm = (current_price - recent_prices.min()) / (recent_prices.max() - recent_prices.min())\n",
    "            \n",
    "            rsi_norm = df.iloc[i]['rsi'] / 100.0\n",
    "            ema_diff = df.iloc[i]['ema_diff']\n",
    "            pos_state = 0\n",
    "            bal_norm = balance / initial_balance\n",
    "            \n",
    "            state_vector = np.array([\n",
    "               close_norm, rsi_norm, ema_diff, prob, pos_state, bal_norm, 0\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            if state_vector.shape != (7,):\n",
    "                 state_vector = np.zeros(7, dtype=np.float32)\n",
    "            \n",
    "            # Trinity Decision\n",
    "            action_code, confidence = engine.decide_action(prob, state_vector, current_candles)\n",
    "            \n",
    "            if \"BUY\" in action_code and balance > 10:\n",
    "                # Extract Leverage if present (e.g. BUY_3x)\n",
    "                leverage = 1\n",
    "                if \"_\" in action_code:\n",
    "                     try:\n",
    "                         leverage = int(action_code.split(\"_\")[1].replace(\"x\",\"\"))\n",
    "                     except:\n",
    "                         leverage = 1\n",
    "                \n",
    "                # Filter weak confidence\n",
    "                if confidence > 55:\n",
    "                    buy_val = balance\n",
    "                    fee = buy_val * fee_rate\n",
    "                    net_buy = buy_val - fee\n",
    "                    \n",
    "                    position = net_buy / current_price\n",
    "                    balance = 0\n",
    "                    entry_price = current_price\n",
    "                    \n",
    "                    trades.append({\n",
    "                        \"time\": timestamp, \"type\": f\"BUY ({leverage}x Signal)\",\n",
    "                        \"price\": current_price, \"confidence\": confidence,\n",
    "                        \"balance\": balance, \"reason\": f\"Trinity {action_code} ({confidence}%)\"\n",
    "                    })\n",
    "\n",
    "        # TRACK EQUITY\n",
    "        current_val = balance + (position * current_price) if position > 0 else balance\n",
    "        equity_history.append(current_val)\n",
    "\n",
    "    # Final Value\n",
    "    final_equity = balance + (position * df.iloc[-1]['close'])\n",
    "    if equity_history:\n",
    "        equity_history[-1] = final_equity\n",
    "    \n",
    "    # Calculate Detailed Metrics\n",
    "    metrics = calculate_detailed_metrics(equity_history, trades, df, initial_balance)\n",
    "    print_metrics_report(metrics)\n",
    "    \n",
    "    return final_equity, trades, df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6305b0",
   "metadata": {},
   "source": [
    "## 3. Start Training\n",
    "Train the models sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4c123ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/IPython/utils/_process_posix.py:125\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     child = \u001b[43mpexpect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-c\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[32m    126\u001b[39m flush = sys.stdout.flush\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/pexpect/pty_spawn.py:205\u001b[39m, in \u001b[36mspawn.__init__\u001b[39m\u001b[34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.use_poll = use_poll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/pexpect/pty_spawn.py:303\u001b[39m, in \u001b[36mspawn._spawn\u001b[39m\u001b[34m(self, command, args, preexec_fn, dimensions)\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.args = [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a.encode(\u001b[38;5;28mself\u001b[39m.encoding)\n\u001b[32m    301\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args]\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28mself\u001b[39m.ptyproc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[38;5;28mself\u001b[39m.ptyproc.pid\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/pexpect/pty_spawn.py:315\u001b[39m, in \u001b[36mspawn._spawnpty\u001b[39m\u001b[34m(self, args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPtyProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/ptyprocess/ptyprocess.py:315\u001b[39m, in \u001b[36mPtyProcess.spawn\u001b[39m\u001b[34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[39m\n\u001b[32m    314\u001b[39m os.close(exec_err_pipe_write)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m exec_err_data = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m os.close(exec_err_pipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython backend/train_cnn.py\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/ipykernel/zmqshell.py:788\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning/predictx/.venv/lib/python3.14/site-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m.sendline(\u001b[38;5;28mchr\u001b[39m(\u001b[32m3\u001b[39m))\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'child' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "!python backend/train_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8b28341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \u001b[35m\"/Users/weka/Learning/predictx/backend/train_rl_agent.py\"\u001b[0m, line \u001b[35m17\u001b[0m\n",
      "    \\\u001b[1;31m\"\u001b[0m\\\"\\\"\n",
      "     \u001b[1;31m^\u001b[0m\n",
      "\u001b[1;35mSyntaxError\u001b[0m: \u001b[35munexpected character after line continuation character\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python backend/train_rl_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ecef0",
   "metadata": {},
   "source": [
    "## 4. Download Trained Models\n",
    "Zip and download the models to your local machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: backend/models/ (stored 0%)\n",
      "updating: backend/models/predictx_v2.pth (deflated 7%)\n",
      "updating: backend/models/ppo_agent.zip (stored 0%)\n",
      "updating: backend/models/cnn_pattern_v2.pth (deflated 9%)\n",
      "updating: backend/models/cnn_pattern_v1.pth (deflated 9%)\n",
      "updating: backend/models/lstm_v1.pth (deflated 8%)\n",
      "updating: backend/models/scaler_v2.pkl (deflated 24%)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mzip -r trained_models.zip backend/models\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      3\u001b[39m files.download(\u001b[33m'\u001b[39m\u001b[33mtrained_models.zip\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!zip -r trained_models.zip backend/models\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
